@misc{Claude,
abstract = {• Claude is powered by a technique called Constitutional AI which aligns language models via natural language feedback. • Conversations with Claude took place over the course of [DATE RANGE] to gain an AI perspective on various concepts and their interconnections. • Claude's knowledge comes only from what it has been exposed to and learned based on public data sources and conversations. There are gaps and limitations in its knowledge and abilities. • Citing an AI system as an author raises some interesting questions. However, Claude is capable of maintaining a consistent persona and line of dialogue, so can be considered a "speaker" for the purposes of these conversations, if not strictly an author. • Anthropic, PBC, the creators of Claude, is an AI safety startup based in San Francisco, founded in 2021.},
annote = {Conversational AI created by Anthropic to be helpful, harmless, and honest.},
author = {Anthropic},
mendeley-groups = {PhD},
title = {{Anthropic AI Assistant}},
url = {https://www.anthropic.ai/},
year = {2021}
}
@article{Bowman2022,
abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
archivePrefix = {arXiv},
arxivId = {2211.03540},
author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v{s}}iūtė, Kamilė and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noem{\'{i}} and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Ben and Kaplan, Jared},
eprint = {2211.03540},
file = {:Users/v1nhood/Library/CloudStorage/Dropbox/Mendeley/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large Language Models(2).pdf:pdf},
mendeley-groups = {PhD},
month = {nov},
title = {{Measuring Progress on Scalable Oversight for Large Language Models}},
url = {https://arxiv.org/abs/2211.03540v2},
year = {2022}
}
@movie{starwarsIV,
mendeley-groups = {PhD},
title = {{Star Wars: Episode IV - A New Hope}},
year = {1977}
}
@video{serra1973,
author = {Serra, Richard},
mendeley-groups = {PhD},
publisher = {Richard Serra},
title = {{Television Delivers People}},
year = {1973}
}
